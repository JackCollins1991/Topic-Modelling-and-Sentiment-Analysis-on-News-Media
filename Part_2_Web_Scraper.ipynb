{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jack_Collins_MA5851_A3_Part1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "I1ufnk--ahT0",
        "4ZV3MugkZGs8",
        "Qj_Zr_ciLQ9Q",
        "c_B-5Je7v1VF",
        "a3sbGiBQgn5E"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-nEWxgR5o9M"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx4U9Zsw8kna"
      },
      "source": [
        "This notebook explains a pipeline for:     \n",
        "* Taking a URL as input\n",
        "* Extracting all URLs found on that web page which match specified criteria\n",
        "* Extracting the text on each of those sub-URLs\n",
        "* Generating a data table of the url, the text and the first title located on that page\n",
        "* Writing this data to a CSV, which is expected to then be used in a subsequent pipeline. \n",
        "\n",
        "This is the abstract task which this pipeline accomplishes. In this scenario, we will configure the pipeline for a specific use case, which is:    \n",
        "* Input the URL for 'The Daily Mail UK' news publication's 'breaking News' page\n",
        "* Extract every news article on the front page of the website\n",
        "* Extract all text from each article, which will then be used in the 'NLP Analysis' notebook.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X6RuZ8zBwrD"
      },
      "source": [
        "## Use Case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaVY7lcDbd7k"
      },
      "source": [
        "In this use case, we are aiming to conduct a topic modelling and sentiment analysis on each article found by web scraping the Daily Mail's 'Breaking News' web page. We are interested to know if certain topics correlate with sentiment, which would indicate especially positive or negative discourse on that topic. \n",
        "\n",
        "We are inputting in only the URL for the 'Breaking News - Daily Mail UK' web page. This  pipeline will then automatically extract all URLs from that webpage and filter in only URLs which appear to be news articles (as opposed to URLs to images or ads).\n",
        "These news articles cover a range of topics including politics, news and entertainment. However, they are all 'current events' stories, meaning they are the 'headline' stories for the particular day this pipeline is run on. Therefore, this pipeline will produce different results when re-run. \n",
        "\n",
        "While each news article web page contains many elements besides the text of the main story, the main story text is generally contained in predictable tags, in this case 'p' for 'paragraph text.' Undesired texts, such as previews for other stories, are usually wrapped in other tags. \n",
        "\n",
        "### Copyright considerations \n",
        "The Daily Mail terms of use allow for non-commercial re-use of partial texts from their articles. The website also does not use cloudflare, which will allow us to conduct web scraping.(Daily Mail, 2011)\n",
        "\n",
        "### Metadata supplementation\n",
        "This pipelines does more than extract links from a webpage, it also extracts the text found on each URL and provides it to us in a data table. We enrich this data in the following ways:    \n",
        "* Text cleaning by removing the following unwanted characters. Further cleaning is done in the subsequent pipeline with stop words and removing infrequent words which may be the result of residual html in the text strings. \n",
        "\n",
        "* removed substrings are: \"]\",'\"',\"'\",\".\",\",\",\"[\",\"/\",\">\",\"<\"\n",
        "\n",
        "* In addition to extracting the raw text in the article, we also extract the title of the article by finding the first 'title' tag that occurs on the page. \n",
        "\n",
        "* The resultant data table of URL, text, title, are then stored in an output file. That file can then be consumed by the subsequent NLP pipeline. \n",
        "\n",
        "### Demonstration\n",
        "\n",
        "What follows is a demonstration of setting up the required configurations for the pipeline (see the 'Configuration' section) and then executing the pipeline with those configurations (see 'Execution' section). \n",
        "\n",
        "After executing the pipeline, we can also briefly review the data that had been outputted. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17l42JapjFN2"
      },
      "source": [
        "## Extending and Scaling this Prototype\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiLWCmvBjJ2z"
      },
      "source": [
        "This pipeline contains functions and a template for a pipeline which can be used to accomplish this abstracted use case. This is intended as a prototype which could later be developed as:       \n",
        "* An application for extracting text from news articles on a series of news websites. This would require altering the current method which only evaluates URLs from the inputted webpage. This new implementation would extract every URL present on several domains which meet a specified criteria. \n",
        "* Extracting news articles from across a time range, instead of just the news of today. \n",
        "* This example will collected around 100-200 articles and takes approximately 2.5 minutes to run. The majority of computation time is from extracting text from the larger HTML text blobs. To scale the application to many thousands or millions of articles, could be accomplished by distributing out the processing for each sub-URL across parallel computers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsL5839WZDuh"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd-mK_jbI4Pq"
      },
      "source": [
        "In the following section, we:      \n",
        "* Import required modules\n",
        "* Set the configuration constants, including the target URLs. This pipeline should be capable of performing the same task for different use cases by changing only the configurations. No changes to the code are required. \n",
        "* Define the functions for use in the 'Execution' section.\n",
        "* Connect this runtime to file storage (Google Drive) to store the output file. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1ufnk--ahT0"
      },
      "source": [
        "## Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea7sAKQeZSbL"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pprint import pprint\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3B0FUMbaklt"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTHjLFjUL4iv"
      },
      "source": [
        "The constants are the runtime variables which can be set here in this one code chunk. Future versions of this application could set these variables from an external config file. By changing the constants here, this pipeline can accomplish its task on a different use case, without any need to change code. \n",
        "These constants are:     \n",
        "\n",
        "* MAIN_URL - This is the inputted webpage from which we will extract the URLS found there. An example would be the home page of a news website. \n",
        "* DOMAIN - URLs on pages are often partial urls and we will append this domain to the start to create the full URL. \n",
        "* SCRAPE_OUTPUT_FILE - File location for the output file. \n",
        "* ARTICLE_TAG = - the html tag we can use to identify when an element contains a URL we are interested in. For example 'a' tags contain articles, while other tags may contain URLS to ad sites. \n",
        "* URL_HTML_TAG - Th etag which indicates the URL, this is commonly 'href'\n",
        "* URLS_START_WITH - Filters in only URLS that start with this substring. \n",
        "* URLS_NOT_END_WITH - FIlters out URLs that end with the substring. \n",
        "* TEXT_TAG - The HTML tag which indicates the body text we want to extract. \n",
        "* REMOVE_SUBSTRINGS List of characters and substrings which can be cleaned out of the extracted text. \n",
        "* SEED - The random seed to be set for reproduceability. \n",
        "\n",
        "The HTML tags and URL substrings to filter on, are determined by examining the HTML for the target pages directly. This will be demonstrated in the 'Execution' section below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__m-1cnjY94B"
      },
      "source": [
        "MAIN_URL = 'https://www.dailymail.co.uk/news/breaking_news/index.html'\n",
        "DOMAIN = 'https://www.dailymail.co.uk'\n",
        "SCRAPE_OUTPUT_FILE = '/content/drive/MyDrive/MA5851_A3/scrape_results.csv'\n",
        "ARTICLE_TAG = 'a'\n",
        "URL_HTML_TAG = 'href'\n",
        "URLS_START_WITH = ['https://www.dailymail.co.uk']\n",
        "URLS_NOT_END_WITH = ['#video']\n",
        "TEXT_TAG = 'p'\n",
        "REMOVE_SUBSTRINGS = [\"]\",'\"',\"'\",\".\",\",\",\"[\",\"/\",\">\",\"<\"]\n",
        "SEED = 42"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPqxXpP8mJJ5"
      },
      "source": [
        "## File Storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "a8TwB-0LmFQe",
        "outputId": "8cf00fe1-14b4-4e86-8265-1ae525097ecc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZV3MugkZGs8"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjOYu0SeTCG0"
      },
      "source": [
        "Where the imported modules' functionality isn't precisley suited for our use cases, it is appropriate to define some custome functions. These functions are re-useable for other use cases. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILtxJO7JeW5m"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPVJ0UnueZgd"
      },
      "source": [
        "def Set_All_Seeds(seed):\n",
        "  \"\"\"Aims to set all used random seeds in one place.\"\"\"\n",
        "  random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  np.random.seed(seed)\n",
        "  np.random.RandomState(seed)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj_Zr_ciLQ9Q"
      },
      "source": [
        "### HTML Element Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC7xdREIenUG"
      },
      "source": [
        "def Get_Soup(url: str):\n",
        "  \"\"\"Input a url and return the BeautifulSoup instance (aka: a 'soup').\"\"\"\n",
        "  data = requests.get(url)\n",
        "  html = BeautifulSoup(data.text, 'html.parser')\n",
        "  return html"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z5nv78ojRcZ"
      },
      "source": [
        "def Get_Links(soup: BeautifulSoup, find_tag: str, get_tag: str):\n",
        "  \"\"\"Extracts every URL found in the 'get_tag' of each 'find_tag' in the soup.\"\"\" \n",
        "  results = []\n",
        "  for link in soup.find_all(find_tag):\n",
        "    results.append(link.get(get_tag))\n",
        "  return results"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Neki7NkjR7Bs"
      },
      "source": [
        "def Get_Content(soup: BeautifulSoup, tag: str):\n",
        "  \"\"\"Input a soup, return a list of strings which are the \n",
        "  contents found between each tag\"\"\"\n",
        "  results = []\n",
        "  for p in soup.find_all(tag):\n",
        "    results.append(p.contents)\n",
        "  return results"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3sbGiBQgn5E"
      },
      "source": [
        "### Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1enatLvguPO"
      },
      "source": [
        "def Remove_Sub_Strings(string: str, remove: list):\n",
        "  \"\"\"User can input a list of substrings which will all be \n",
        "  removed from the string\"\"\"\n",
        "  for r in remove:\n",
        "    assert isinstance(string, str)\n",
        "    string = string.replace(r,\"\")\n",
        "  return string\n",
        "\n",
        "def Clean_String(s: str):\n",
        "  \"\"\"Cleans regex characters from string\"\"\"\n",
        "  s = re.compile(r'<[^>]+>')\n",
        "  return re.sub('(^|\\s+)FIRST($|\\s+)', '', s)\n",
        "\n",
        "def Get_Text_From_Page(url: str):\n",
        "  \"\"\"Input a url and returns only the text found on the page.\n",
        "  Reuires runtime variable 'TEXT_TAG' and 'REMOVE_SUBSTRINGS' to be defined.\"\"\"\n",
        "  web_text = Get_Content(soup = Get_Soup(url), tag=TEXT_TAG)\n",
        "  return Remove_Sub_Strings(str(web_text), remove = REMOVE_SUBSTRINGS)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_B-5Je7v1VF"
      },
      "source": [
        "### Link Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnwGCEtYolD2"
      },
      "source": [
        "def Select_Links_Starts_With(links: list, stem: str):\n",
        "  \"\"\"Input a list of URLS, returns the URLs which start with the stem\"\"\"\n",
        "  results = []\n",
        "  for link in links:\n",
        "    if not isinstance(link, str):\n",
        "      continue\n",
        "    if link.startswith(stem):\n",
        "      results.append(link)\n",
        "  return results\n",
        "\n",
        "def Select_Links_Ends_With(links: list, stem: str):\n",
        "  \"\"\"Input a list of URLS, returns the URLs which end with the stem\"\"\"\n",
        "  results = []\n",
        "  for link in links:\n",
        "    if not isinstance(link, str):\n",
        "      continue\n",
        "    if link.endswith(stem):\n",
        "      results.append(link)\n",
        "  return results\n",
        "\n",
        "def Remove_Links(func, links: list, stems: list):\n",
        "  \"\"\"Allows users to filter out URLs with a list of stems.\"\"\"\n",
        "  for s in stems:\n",
        "    delta = func(links = links, stem=s)\n",
        "    links = list(set(links) - set(delta))\n",
        "  return links\n",
        "\n",
        "def Append_Links(func, links: list, stems: list):\n",
        "  \"\"\"Allows users to filter in URLs with a list of stems.\"\"\"\n",
        "  results = []\n",
        "  for s in stems:\n",
        "    delta = func(links = links, stem=s)\n",
        "    results.append(delta)\n",
        "  return results[0]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spLnfetLv8r1"
      },
      "source": [
        "# Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-_p1My-efV5"
      },
      "source": [
        "Set_All_Seeds(SEED)\n",
        "start_time = time.time()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVM-P8o5WxpC"
      },
      "source": [
        "## Extract webpages\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjXZ0CP44K9J"
      },
      "source": [
        "# Take the main URL and extract all desired URLs found on that webpage into a list. \n",
        "main_soup = Get_Soup(MAIN_URL)\n",
        "URLs = Get_Links(soup = main_soup, find_tag = ARTICLE_TAG, get_tag = URL_HTML_TAG)\n",
        "URLs = Append_Links(func = Select_Links_Starts_With,links = URLs, stems = URLS_START_WITH)\n",
        "URLs = Remove_Links(func = Select_Links_Ends_With,links = URLs, stems = URLS_NOT_END_WITH)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FPy6-NMYZwU"
      },
      "source": [
        "We can examine a sample of the main page HTML code here. From exploring the full HTML document, we can determine the tags and url stems needed to extract the desired URLS and set those porperties in the configuration. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "TfbKxq8OYV0u",
        "outputId": "85ae9d49-3305-42ab-d701-9d041b4741e8"
      },
      "source": [
        "str(main_soup)[0:300]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"//www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\\n<html><head><script type=\"text/javascript\">\\ntry {\\n  Object.defineProperty(window, \\'adverts\\', {configurable: false, value:{}});\\n}\\ncatch(error) {\\n  console.error(error);\\n}\\n</script><lin'"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UOSUw7aYokz"
      },
      "source": [
        "Using the tags and stems we have inputted into the configuration, we can extract a list of URLs as follows. The number of URLs extracted will be detailed in the output section below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cFDtKUXeYv6J",
        "outputId": "a0c18b75-4b96-4f65-f104-1acff7299063"
      },
      "source": [
        "URLs[0:4]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://www.dailymail.co.uk/news/article-10270907/Norway-Covid-60-infected-Omicron-Christmas-party-mild-symptoms.html#comments',\n",
              " 'https://www.dailymail.co.uk/news/article-10274357/Australian-cricket-coach-Justin-Langer-Tim-Paine-sexting-scandal.html',\n",
              " 'https://www.dailymail.co.uk/health/article-10268047/Second-case-Omicron-COVID-19-variant-detected-Minnesota.html',\n",
              " 'https://www.dailymail.co.uk/news/article-10272445/Biden-says-putting-plans-deter-Putin-invading-Ukraine.html']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwyP72T-Y2yn"
      },
      "source": [
        "We can apply our custom function to extract all body text for each URL in the list. See in the below example, that some html code and unwanted sub strings are still in the text. These will be removed with stop words in the subsequent pipeline.' "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "Qr0pefA4ikcn",
        "outputId": "eb392ffe-e4ff-498f-a9dd-90912dc7e8a5"
      },
      "source": [
        "#First 600 characters of article htmls are unhelful matter.\n",
        "Get_Text_From_Page(URLs[1])[725:]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'30 View  br  comments  Australian cricket coach Justin Langer has gone in to bat for former captain Tim Paine and knocked critics for six in the wake of the ex-skippers sexting scandal\\\\xa0 Langer came out on the front foot to smash the relentless attacks on Paine as a hypocritical pursuit of false perfectionism Our captain one of the best made a mistake and is paying a heavy price for it Langer slammed ahead of the first Ashes Test at the Gabba on Wednesday\\\\xa0 What I continually see in this job and see in the society we live in – its brutal We live in a world of perfectionism dont we? We are a very judgmental society\\\\xa0 Australian cricket coach Justin Langer (pictured) has slammed the attacks on Tim Paine who stood down as Test captain in November after a sexting scandal from 2017 was made public Justin Langer confirmed Tim Paines priority is his family following the highly publicised sexting scandal which resulted in the wicketkeeper (pictured here with wife Bonnie)\\\\xa0 taking an indefinite break from the sport Langer was appointed coach in 2018 in the wake of the sandpapergate scandal in South Africa and he said he saw similarities in the outrage that have followed both As I said in my very first press conference in 2018 when I was asked about Steve Smith and David Warner and Cameron Bancroft theres not one person who is asking questions here or who is on the camera here or who is listening to this or watching who hasnt made a mistake in their life he said Theres not a single person You learn your lessons but we live in an unforgiving society And thats a shame Langer still believes Paine may one day return to the Australian Test side He (Paine) loves cricket He absolutely loves cricket he said\\\\xa0 And hes 37 He is a fit as any athlete certainly in our squad He looks after himself so well\\\\xa0 Hes very focused so who knows His number one priority at the moment is family as you can imagine and thats how it should be Im not sure weve seen the end of him But well wait and see Thatll be his decision Langer said Paine remained one of my really close friends and someone I admire enormously But he acknowledged the life of the wicketkeeper had changed dramatically in recent weeks amid concerns for the fallen heros mental health The passionate defence comes after Langer flew from Brisbane to Hobart late last month to personally check on the welfare of his former skipper Paine\\\\xa0resigned from the prized position on November 19 when it emerged he sent sexually explicit messages to an employee of Cricket Tasmania in 2017 Cricket Tasmania became aware of the texts in mid-2018 with an investigation by the organisation and Cricket Australia clearing him of any code of conduct breaches\\\\xa0 Paine then announced he was taking an indefinite break from the sport Tim Paine\\\\xa0 (left) was recently visited by coach Justin Langer (right with Paine) who flew to Hobart from Brisbane to see first-hand the mental state of his former skipper Langer and Paine worked closely together to rebuild the national team after the 2018 ball tampering scandal\\\\xa0\\\\xa0\\\\xa0\\\\xa0 Paine told newscomau that Langer wanted him to remain as captain JL (Justin Langer) told me hes devastated Paine said He was pretty firm that he wanted me to continue as captain and again once I explained to him the reasons that I thought resigning was the best thing to do he was with me all the way\\\\xa0 Paine 36 sent a photo of his penis to a female co-worker along with a stream of lewd text messages in 2017 Teammates also reportedly still wanted the veteran wicketkeeper to play the first Ashes Test at the Gabba against England beginning on Wednesday  Cricket\\\\xa0 a class=class href=newstasmaniaindexhtml id=mol-f1776110-517c-11ec-86cb-ed6d7a78f92a style=font-weight: bold;Tasmaniaa \\\\xa0then confirmed he wouldnt play in the recent one-dayer versus Western Australia on November 26 with his\\\\xa0manager James Henderson later stating Paine was stepping away from cricket for an indefinite mental health break He posted on  a class=class href=https:twittercomjahenderson63status1464004967459483651 rel=nofollow noreferrer noopener style=font-weight: bold; target=_blankTwittera \\\\xa0that there were extreme concerns for the well-being of Paine and his wife Bonnie\\\\xa0 Paine was investigated by Cricket Australia in 2018 after former Cricket Tasmania staffer Renee Ferguson complained but cleared of misconduct and the matter was kept secret He sent a photo of his penis to the female co-worker along with a stream of lewd text messages many of which are too raunchy to publish Will you want to taste my d**?? F**k me Im seriously hard one of the messages sent to the Cricket Tasmania employee read The lewd messages were sent\\\\xa0on November 22 and 23 2017 the eve and morning of the first Ashes Test at the Gabba in Brisbane\\\\xa0 Were both f**ked if this got out Ms Ferguson wrote in one text to Paine Despite this she complained in 2018 about Mr Paines sexually explicit unwelcome and unsolicited photograph of his genitals in addition to the graphic sexual comments Bonnie Paine has stood by her husband after the sexting scandal went public (pictured together in 2020 at the Australian Cricket Awards at the Crown Palladium in Melbourne) Tim Paine and coach Justin Langer (pictured) forged a close bond after the wicketkeeper became Test skipper in 2018\\\\xa0 She is said to have been taken aback by the comments and pornographic photo which she found offensive However the complaints were only made about seven months later when Ms Ferguson was\\\\xa0charged with stealing Those charges are still before the courts Cricket Tasmania chairman Andrew Gaggin said Ms Ferguson was earlier sacked by the organisation but did not bring the messages up at the time Ms Ferguson interacted with Paine on and off for about a year and both parties were flirtatious in the leaked messages But conversation took a sudden turn with the cricketers explicit messages just before she resigned from Cricket Tasmania She sent a letter to Cricket Australia and Cricket Tasmania as well as the\\\\xa0Australian Human Rights Commission complaining of sexual harassment\\\\xa0 Ms Ferguson\\\\xa0has also filed a bombshell sexual harassment claim in Federal Court but her lawsuit does not relate to Paine in any way\\\\xa0instead to another man in the Cricket Tasmania office Her claims she was subjected to sexual banter at Cricket Tasmania and alleges she was told to get on snap Snapchat you mole Bowler Pat Cummins (pictured) will now lead out Australia in the first Ashes Test from December 8 in Brisbane after Tim Paine stepped aside following the sexting scandal Paines wife Bonnie (pictured) was aware of the sexting scandal from 2017 - but chose to stand by her husband Tim Paine (pictured) was in tears as he announced his resignation as Australias Test captain in a statement on November 19  Paine\\\\xa0announced his resignation in a statement on November 19 and held a press conference where he tearfully apologised Its an incredibly difficult decision but the right one for me my family and cricket he said Nearly four years ago I was involved in a text exchange with a then-colleague At the time the exchange was the subject of a thorough CA Integrity Unit investigation throughout which I fully participated in and openly participated in That investigation and a Cricket Tasmania HR investigation at the same time found that there had been no breach of the Cricket Australia Code of Conduct Although exonerated I deeply regretted this incident at the time and still do today I spoke to my wife and family at the time and am enormously grateful for their forgiveness and support Mrs Paine who found out about the texts at the time said she forgave her husband and claimed that the incident made their marriage stronger Although I had mixed emotions and I felt betrayed and I felt hurt upset and I felt pissed off I also had feelings of gratitude because he was being so honest with me I thought OK do I walk? Or forgive and rebuild? I chose the latter she told the  a class=class href=https:wwwheraldsuncomaunewsvictoriahamish-mclachlan-tim-paine-and-wife-bonnie-say-sexting-scandal-helped-their-marriagenews-story9b817d23d7e8041e18b8c3f02eee522a rel=nofollow noreferrer noopener style=font-weight: bold; target=_blankHerald Suna  after the scandal was made public Australian captain Tim Paine walks to change ends during a cricket test against India at the Sydney Cricket Ground in Sydney Australia on Jan 9 2021 2010: Tim Paine makes his Test debut against Pakistan replacing injured Brad Haddin but is dumped upon the incumbents return April 2016: Paine marries Bonnie Maggs November 17 2017: Granted shock recall for Ashes series November 22-23 2017: Paine sends lewd messages to a female coworker on the eve and morning of the first Ashes Test in Brisbane The pair had exchanged texts throughout the year January 2018: Selected for squad to tour South Africa March 25 2018: Appointed interim captain after Steve Smith and David Warner stood down over sandpapergate ball tampering scandal March 28 2018: Paine is appointed captain for the 4th Test of the series becoming the 46th captain of the Australian side April 2018: Awarded a national contract\\\\xa0by Cricket Australia\\\\xa0 June 2018: Cricket Australia and Cricket Tasmania become aware of the messages and launch an investigation following a complaint from the woman Paine claims he was exonerated during the investigation His wife Bonnie was aware of the messages but chose to stick by him 2018 - 2021: Paine continues as Test captain retaining the Ashes in England in 2019 November 19 2021: Paine steps down as full details of the explicit messages surface Published by Associated Newspapers Ltd Part of the Daily Mail The Mail on Sunday & Metro Media Group'"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f6anWkAW07W"
      },
      "source": [
        "## Extend the metadata with new features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQa-kkincda7"
      },
      "source": [
        "From the list of URLs, we can develop more features and output a data table of:    \n",
        "* The URL\n",
        "* The title of the article\n",
        "* 'bag of words' which is the text in the article. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bRDAryGSAaH"
      },
      "source": [
        "bag_of_words = []\n",
        "for url in URLs:\n",
        "  bag_of_words.append(Get_Text_From_Page(url)[600:])\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hWDNSmYAcC0"
      },
      "source": [
        "titles = []\n",
        "for url in URLs:\n",
        "  titles.append(Get_Soup(url).find(\"title\").contents[0])"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEu3r3tsxeJB"
      },
      "source": [
        "output = pd.DataFrame({\"URLS\":URLs,\"Bag Of Words\":bag_of_words,\"Title\":titles}).drop_duplicates()\n",
        "output.to_csv(SCRAPE_OUTPUT_FILE)\n",
        "execution_time = time.time() - start_time"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itKGf1hX7fUA"
      },
      "source": [
        "## Output Profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rB2ZwFWc6Q6"
      },
      "source": [
        "The data table has been written to the output file. We can finish this pipeline by providing some descriptive information about the corpus. The following is a preview of the data's first three rows. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "J_LYa4QZdXRs",
        "outputId": "fc786a4a-fccf-4a28-a01d-d0c25e8124fc"
      },
      "source": [
        "output.head(3)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>URLS</th>\n",
              "      <th>Bag Of Words</th>\n",
              "      <th>Title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.dailymail.co.uk/news/article-10270...</td>\n",
              "      <td>Updated:span time datetime=2021-12-03T08:48:04...</td>\n",
              "      <td>Norway Covid: 60 infected with Omicron at Chri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.dailymail.co.uk/news/article-10274...</td>\n",
              "      <td>pan class=article-timestamp-labelUpdated:span ...</td>\n",
              "      <td>Australian cricket coach Justin Langer on Tim ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.dailymail.co.uk/health/article-102...</td>\n",
              "      <td>span time datetime=2021-12-02T22:12:24+0000 17...</td>\n",
              "      <td>Second US case of Omicron COVID-19 variant det...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                URLS  ...                                              Title\n",
              "0  https://www.dailymail.co.uk/news/article-10270...  ...  Norway Covid: 60 infected with Omicron at Chri...\n",
              "1  https://www.dailymail.co.uk/news/article-10274...  ...  Australian cricket coach Justin Langer on Tim ...\n",
              "2  https://www.dailymail.co.uk/health/article-102...  ...  Second US case of Omicron COVID-19 variant det...\n",
              "\n",
              "[3 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcfHQbLmfbyS"
      },
      "source": [
        "Exploratory data analysis will be conducted on the corpus in the subsequent pipeline which performs NLP analysis. This pipeline is only responsible for extracting the text from the web pages. However, to ensure quaity data is sent to the next pipeline, we can explore:     \n",
        "* The number of articles extracted\n",
        "* Execution time\n",
        "* Check for rows missing text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMsvqEvTeVkv"
      },
      "source": [
        "The total number of articles extracted:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VchTU15I_9Hc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39caf9b1-ad70-4376-8699-cabf359790fc"
      },
      "source": [
        "len(output)\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "155"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thDXzgFFeg9S"
      },
      "source": [
        "The time taken to process this many articles (in seconds):     "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK6KYkGc0FBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "767b27f0-29c1-4672-cdc2-4dff45a2bfbc"
      },
      "source": [
        "execution_time "
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5243.575374126434"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6NWyuNog3hn"
      },
      "source": [
        "Low word counts indicate a problem extracting text from HTML. We can check how many records have a small word count as follows. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m1ydElvhLw8",
        "outputId": "60ce1355-8623-43d8-9a57-1bfe992c11c9"
      },
      "source": [
        "ind = output[\"Bag Of Words\"].str.len() < 100\n",
        "len(output[ind])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xZobn74bpFD"
      },
      "source": [
        "# Refferences\n",
        "\n",
        "Reporter, A. Daily Mail. “Daily Mail - Terms.” Mail Online, June 7, 2011. https://www.dailymail.co.uk/home/article-1388146/Terms.html."
      ]
    }
  ]
}