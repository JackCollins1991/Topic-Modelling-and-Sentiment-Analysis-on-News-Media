{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jack_Collins_MA5851_A3_Part1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "I1ufnk--ahT0",
        "4ZV3MugkZGs8",
        "Qj_Zr_ciLQ9Q",
        "c_B-5Je7v1VF",
        "a3sbGiBQgn5E"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-nEWxgR5o9M"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx4U9Zsw8kna"
      },
      "source": [
        "This notebook explains a pipeline for:\n",
        "\n",
        "* Taking a URL as input\n",
        "* Extracting all URLs found on that web page which match specified criteria\n",
        "* Extracting the text on each of those sub-URLs\n",
        "* Generating a data table of the url, the text and the first title located on that page\n",
        "* Writing this data to a CSV, which is expected to then be used in a subsequent pipeline. \n",
        "\n",
        "This is the abstract task which this pipeline accomplishes. In this scenario, we will configure the pipeline for a specific use case, which is:\n",
        "\n",
        "* Input the URL for 'The Daily Mail UK' news publication's 'breaking News' page\n",
        "* Extract every news article on the front page of the website\n",
        "* Extract all text from each article, which will then be used in the 'NLP Analysis' notebook.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X6RuZ8zBwrD"
      },
      "source": [
        "## Use Case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaVY7lcDbd7k"
      },
      "source": [
        "In this use case, we are aiming to conduct a topic modelling and sentiment analysis on each article found by web scraping the Daily Mail's 'Breaking News' web page. We are interested to know if certain topics correlate with sentiment, which would indicate especially positive or negative discourse on that topic. \n",
        "\n",
        "We are inputting in only the URL for the 'Breaking News - Daily Mail UK' web page. This  pipeline will then automatically extract all URLs from that webpage and filter in only URLs which appear to be news articles (as opposed to URLs to images or ads).\n",
        "These news articles cover a range of topics including politics, news and entertainment. However, they are all 'current events' stories, meaning they are the 'headline' stories for the particular day this pipeline is run on. Therefore, this pipeline will produce different results when re-run. \n",
        "\n",
        "While each news article web page contains many elements besides the text of the main story, the main story text is generally contained in predictable tags, in this case 'p' for 'paragraph text.' Undesired texts, such as previews for other stories, are usually wrapped in other tags. \n",
        "\n",
        "### Copyright considerations \n",
        "The Daily Mail terms of use allow for non-commercial re-use of partial texts from their articles. The website also does not use cloudflare, which will allow us to conduct web scraping.(Daily Mail, 2011)\n",
        "\n",
        "### Metadata supplementation\n",
        "This pipelines does more than extract links from a webpage, it also extracts the text found on each URL and provides it to us in a data table. We enrich this data in the following ways:    \n",
        "* Text cleaning by removing the following unwanted characters. Further cleaning is done in the subsequent pipeline with stop words and removing infrequent words which may be the result of residual html in the text strings. \n",
        "\n",
        "* removed substrings are: \"]\",'\"',\"'\",\".\",\",\",\"[\",\"/\",\">\",\"<\"\n",
        "\n",
        "* In addition to extracting the raw text in the article, we also extract the title of the article by finding the first 'title' tag that occurs on the page. \n",
        "\n",
        "* The resultant data table of URL, text, title, are then stored in an output file. That file can then be consumed by the subsequent NLP pipeline. \n",
        "\n",
        "### Demonstration\n",
        "\n",
        "What follows is a demonstration of setting up the required configurations for the pipeline (see the 'Configuration' section) and then executing the pipeline with those configurations (see 'Execution' section). \n",
        "\n",
        "After executing the pipeline, we can also briefly review the data that had been outputted. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17l42JapjFN2"
      },
      "source": [
        "## Extending and Scaling this Prototype\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiLWCmvBjJ2z"
      },
      "source": [
        "This pipeline contains functions and a template for a pipeline which can be used to accomplish this abstracted use case. This is intended as a prototype which could later be developed as:\n",
        "\n",
        "* An application for extracting text from news articles on a series of news websites. This would require altering the current method which only evaluates URLs from the inputted webpage. This new implementation would extract every URL present on several domains which meet a specified criteria. \n",
        "\n",
        "* Extracting news articles from across a time range, instead of just the news of today. \n",
        "\n",
        "* This example will collected around 100-200 articles and takes approximately 2.5 minutes to run. The majority of computation time is from extracting text from the larger HTML text blobs. To scale the application to many thousands or millions of articles, could be accomplished by distributing out the processing for each sub-URL across parallel computers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsL5839WZDuh"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd-mK_jbI4Pq"
      },
      "source": [
        "In the following section, we:\n",
        "\n",
        "* Import required modules\n",
        "\n",
        "* Set the configuration constants, including the target URLs. This pipeline should be capable of performing the same task for different use cases by changing only the configurations. No changes to the code are required. \n",
        "\n",
        "* Define the functions for use in the 'Execution' section.\n",
        "\n",
        "* Connect this runtime to file storage (Google Drive) to store the output file. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1ufnk--ahT0"
      },
      "source": [
        "## Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea7sAKQeZSbL"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pprint import pprint\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3B0FUMbaklt"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTHjLFjUL4iv"
      },
      "source": [
        "The constants are the runtime variables which can be set here in this one code chunk. Future versions of this application could set these variables from an external config file. By changing the constants here, this pipeline can accomplish its task on a different use case, without any need to change code. \n",
        "These constants are:\n",
        "\n",
        "* MAIN_URL - This is the inputted webpage from which we will extract the URLS found there. An example would be the home page of a news website. \n",
        "* DOMAIN - URLs on pages are often partial urls and we will append this domain to the start to create the full URL. \n",
        "* SCRAPE_OUTPUT_FILE - File location for the output file. \n",
        "* ARTICLE_TAG = - the html tag we can use to identify when an element contains a URL we are interested in. For example 'a' tags contain articles, while other tags may contain URLS to ad sites. \n",
        "* URL_HTML_TAG - Th etag which indicates the URL, this is commonly 'href'\n",
        "* URLS_START_WITH - Filters in only URLS that start with this substring. \n",
        "* URLS_NOT_END_WITH - FIlters out URLs that end with the substring. \n",
        "* TEXT_TAG - The HTML tag which indicates the body text we want to extract. \n",
        "* REMOVE_SUBSTRINGS List of characters and substrings which can be cleaned out of the extracted text. \n",
        "* SEED - The random seed to be set for reproduceability. \n",
        "\n",
        "The HTML tags and URL substrings to filter on, are determined by examining the HTML for the target pages directly. This will be demonstrated in the 'Execution' section below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__m-1cnjY94B"
      },
      "source": [
        "MAIN_URL = 'https://www.dailymail.co.uk/news/breaking_news/index.html'\n",
        "DOMAIN = 'https://www.dailymail.co.uk'\n",
        "SCRAPE_OUTPUT_FILE = '/content/drive/MyDrive/MA5851_A3/scrape_results.csv'\n",
        "ARTICLE_TAG = 'a'\n",
        "URL_HTML_TAG = 'href'\n",
        "URLS_START_WITH = ['https://www.dailymail.co.uk']\n",
        "URLS_NOT_END_WITH = ['#video']\n",
        "TEXT_TAG = 'p'\n",
        "REMOVE_SUBSTRINGS = [\"]\",'\"',\"'\",\".\",\",\",\"[\",\"/\",\">\",\"<\"]\n",
        "SEED = 42"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPqxXpP8mJJ5"
      },
      "source": [
        "## File Storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "a8TwB-0LmFQe",
        "outputId": "8cf00fe1-14b4-4e86-8265-1ae525097ecc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZV3MugkZGs8"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjOYu0SeTCG0"
      },
      "source": [
        "Where the imported modules' functionality isn't precisley suited for our use cases, it is appropriate to define some custome functions. These functions are re-useable for other use cases. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILtxJO7JeW5m"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPVJ0UnueZgd"
      },
      "source": [
        "def Set_All_Seeds(seed):\n",
        "  \"\"\"Aims to set all used random seeds in one place.\"\"\"\n",
        "  random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  np.random.seed(seed)\n",
        "  np.random.RandomState(seed)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj_Zr_ciLQ9Q"
      },
      "source": [
        "### HTML Element Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC7xdREIenUG"
      },
      "source": [
        "def Get_Soup(url: str):\n",
        "  \"\"\"Input a url and return the BeautifulSoup instance (aka: a 'soup').\"\"\"\n",
        "  data = requests.get(url)\n",
        "  html = BeautifulSoup(data.text, 'html.parser')\n",
        "  return html"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z5nv78ojRcZ"
      },
      "source": [
        "def Get_Links(soup: BeautifulSoup, find_tag: str, get_tag: str):\n",
        "  \"\"\"Extracts every URL found in the 'get_tag' of each 'find_tag' in the soup.\"\"\" \n",
        "  results = []\n",
        "  for link in soup.find_all(find_tag):\n",
        "    results.append(link.get(get_tag))\n",
        "  return results"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Neki7NkjR7Bs"
      },
      "source": [
        "def Get_Content(soup: BeautifulSoup, tag: str):\n",
        "  \"\"\"Input a soup, return a list of strings which are the \n",
        "  contents found between each tag\"\"\"\n",
        "  results = []\n",
        "  for p in soup.find_all(tag):\n",
        "    results.append(p.contents)\n",
        "  return results"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3sbGiBQgn5E"
      },
      "source": [
        "### Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1enatLvguPO"
      },
      "source": [
        "def Remove_Sub_Strings(string: str, remove: list):\n",
        "  \"\"\"User can input a list of substrings which will all be \n",
        "  removed from the string\"\"\"\n",
        "  for r in remove:\n",
        "    assert isinstance(string, str)\n",
        "    string = string.replace(r,\"\")\n",
        "  return string\n",
        "\n",
        "def Clean_String(s: str):\n",
        "  \"\"\"Cleans regex characters from string\"\"\"\n",
        "  s = re.compile(r'<[^>]+>')\n",
        "  return re.sub('(^|\\s+)FIRST($|\\s+)', '', s)\n",
        "\n",
        "def Get_Text_From_Page(url: str):\n",
        "  \"\"\"Input a url and returns only the text found on the page.\n",
        "  Reuires runtime variable 'TEXT_TAG' and 'REMOVE_SUBSTRINGS' to be defined.\"\"\"\n",
        "  web_text = Get_Content(soup = Get_Soup(url), tag=TEXT_TAG)\n",
        "  return Remove_Sub_Strings(str(web_text), remove = REMOVE_SUBSTRINGS)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_B-5Je7v1VF"
      },
      "source": [
        "### Link Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnwGCEtYolD2"
      },
      "source": [
        "def Select_Links_Starts_With(links: list, stem: str):\n",
        "  \"\"\"Input a list of URLS, returns the URLs which start with the stem\"\"\"\n",
        "  results = []\n",
        "  for link in links:\n",
        "    if not isinstance(link, str):\n",
        "      continue\n",
        "    if link.startswith(stem):\n",
        "      results.append(link)\n",
        "  return results\n",
        "\n",
        "def Select_Links_Ends_With(links: list, stem: str):\n",
        "  \"\"\"Input a list of URLS, returns the URLs which end with the stem\"\"\"\n",
        "  results = []\n",
        "  for link in links:\n",
        "    if not isinstance(link, str):\n",
        "      continue\n",
        "    if link.endswith(stem):\n",
        "      results.append(link)\n",
        "  return results\n",
        "\n",
        "def Remove_Links(func, links: list, stems: list):\n",
        "  \"\"\"Allows users to filter out URLs with a list of stems.\"\"\"\n",
        "  for s in stems:\n",
        "    delta = func(links = links, stem=s)\n",
        "    links = list(set(links) - set(delta))\n",
        "  return links\n",
        "\n",
        "def Append_Links(func, links: list, stems: list):\n",
        "  \"\"\"Allows users to filter in URLs with a list of stems.\"\"\"\n",
        "  results = []\n",
        "  for s in stems:\n",
        "    delta = func(links = links, stem=s)\n",
        "    results.append(delta)\n",
        "  return results[0]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spLnfetLv8r1"
      },
      "source": [
        "# Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-_p1My-efV5"
      },
      "source": [
        "Set_All_Seeds(SEED)\n",
        "start_time = time.time()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVM-P8o5WxpC"
      },
      "source": [
        "## Extract webpages\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjXZ0CP44K9J"
      },
      "source": [
        "# Take the main URL and extract all desired URLs found on that webpage into a list. \n",
        "main_soup = Get_Soup(MAIN_URL)\n",
        "URLs = Get_Links(soup = main_soup, find_tag = ARTICLE_TAG, get_tag = URL_HTML_TAG)\n",
        "URLs = Append_Links(func = Select_Links_Starts_With,links = URLs, stems = URLS_START_WITH)\n",
        "URLs = Remove_Links(func = Select_Links_Ends_With,links = URLs, stems = URLS_NOT_END_WITH)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FPy6-NMYZwU"
      },
      "source": [
        "We can examine a sample of the main page HTML code here. From exploring the full HTML document, we can determine the tags and url stems needed to extract the desired URLS and set those porperties in the configuration. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "TfbKxq8OYV0u",
        "outputId": "85ae9d49-3305-42ab-d701-9d041b4741e8"
      },
      "source": [
        "str(main_soup)[0:300]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"//www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\\n<html><head><script type=\"text/javascript\">\\ntry {\\n  Object.defineProperty(window, \\'adverts\\', {configurable: false, value:{}});\\n}\\ncatch(error) {\\n  console.error(error);\\n}\\n</script><lin'"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UOSUw7aYokz"
      },
      "source": [
        "Using the tags and stems we have inputted into the configuration, we can extract a list of URLs as follows. The number of URLs extracted will be detailed in the output section below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cFDtKUXeYv6J",
        "outputId": "a0c18b75-4b96-4f65-f104-1acff7299063"
      },
      "source": [
        "URLs[0:4]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://www.dailymail.co.uk/news/article-10270907/Norway-Covid-60-infected-Omicron-Christmas-party-mild-symptoms.html#comments',\n",
              " 'https://www.dailymail.co.uk/news/article-10274357/Australian-cricket-coach-Justin-Langer-Tim-Paine-sexting-scandal.html',\n",
              " 'https://www.dailymail.co.uk/health/article-10268047/Second-case-Omicron-COVID-19-variant-detected-Minnesota.html',\n",
              " 'https://www.dailymail.co.uk/news/article-10272445/Biden-says-putting-plans-deter-Putin-invading-Ukraine.html']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwyP72T-Y2yn"
      },
      "source": [
        "We can apply our custom function to extract all body text for each URL in the list. See in the below example, that some html code and unwanted sub strings are still in the text. These will be removed with stop words in the subsequent pipeline.' "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "Qr0pefA4ikcn",
        "outputId": "8384b3cb-76c6-4b03-e95d-c604a5081f6b"
      },
      "source": [
        "#First 725 characters of article htmls are unhelful matter.\n",
        "Get_Text_From_Page(URLs[1])[725:1500]"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'31 View  br  comments  Australian cricket coach Justin Langer has gone in to bat for former captain Tim Paine and knocked critics for six in the wake of the ex-skippers sexting scandal\\\\xa0 Langer came out on the front foot to smash the relentless attacks on Paine as a hypocritical pursuit of false perfectionism Our captain one of the best made a mistake and is paying a heavy price for it Langer slammed ahead of the first Ashes Test at the Gabba on Wednesday\\\\xa0 What I continually see in this job and see in the society we live in – its brutal We live in a world of perfectionism dont we? We are a very judgmental society\\\\xa0 Australian cricket coach Justin Langer (pictured) has slammed the attacks on Tim Paine who stood down as Test captain in November after a sexting'"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f6anWkAW07W"
      },
      "source": [
        "## Extend the metadata with new features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQa-kkincda7"
      },
      "source": [
        "From the list of URLs, we can develop more features and output a data table of:\n",
        "\n",
        "* The URL\n",
        "* The title of the article\n",
        "* 'bag of words' which is the text in the article. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bRDAryGSAaH"
      },
      "source": [
        "bag_of_words = []\n",
        "for url in URLs:\n",
        "  bag_of_words.append(Get_Text_From_Page(url)[600:])\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hWDNSmYAcC0"
      },
      "source": [
        "titles = []\n",
        "for url in URLs:\n",
        "  titles.append(Get_Soup(url).find(\"title\").contents[0])"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEu3r3tsxeJB"
      },
      "source": [
        "output = pd.DataFrame({\"URLS\":URLs,\"Bag Of Words\":bag_of_words,\"Title\":titles}).drop_duplicates()\n",
        "output.to_csv(SCRAPE_OUTPUT_FILE)\n",
        "execution_time = time.time() - start_time"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itKGf1hX7fUA"
      },
      "source": [
        "## Output Profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rB2ZwFWc6Q6"
      },
      "source": [
        "The data table has been written to the output file. We can finish this pipeline by providing some descriptive information about the corpus. The following is a preview of the data's first three rows. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "J_LYa4QZdXRs",
        "outputId": "fc786a4a-fccf-4a28-a01d-d0c25e8124fc"
      },
      "source": [
        "output.head(3)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>URLS</th>\n",
              "      <th>Bag Of Words</th>\n",
              "      <th>Title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.dailymail.co.uk/news/article-10270...</td>\n",
              "      <td>Updated:span time datetime=2021-12-03T08:48:04...</td>\n",
              "      <td>Norway Covid: 60 infected with Omicron at Chri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.dailymail.co.uk/news/article-10274...</td>\n",
              "      <td>pan class=article-timestamp-labelUpdated:span ...</td>\n",
              "      <td>Australian cricket coach Justin Langer on Tim ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.dailymail.co.uk/health/article-102...</td>\n",
              "      <td>span time datetime=2021-12-02T22:12:24+0000 17...</td>\n",
              "      <td>Second US case of Omicron COVID-19 variant det...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                URLS  ...                                              Title\n",
              "0  https://www.dailymail.co.uk/news/article-10270...  ...  Norway Covid: 60 infected with Omicron at Chri...\n",
              "1  https://www.dailymail.co.uk/news/article-10274...  ...  Australian cricket coach Justin Langer on Tim ...\n",
              "2  https://www.dailymail.co.uk/health/article-102...  ...  Second US case of Omicron COVID-19 variant det...\n",
              "\n",
              "[3 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcfHQbLmfbyS"
      },
      "source": [
        "Exploratory data analysis will be conducted on the corpus in the subsequent pipeline which performs NLP analysis. This pipeline is only responsible for extracting the text from the web pages. However, to ensure quaity data is sent to the next pipeline, we can explore:\n",
        "\n",
        "* The number of articles extracted\n",
        "* Execution time\n",
        "* Check for rows missing text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMsvqEvTeVkv"
      },
      "source": [
        "The total number of articles extracted:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VchTU15I_9Hc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39caf9b1-ad70-4376-8699-cabf359790fc"
      },
      "source": [
        "len(output)\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "155"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thDXzgFFeg9S"
      },
      "source": [
        "The time taken to process this many articles (in seconds):     "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK6KYkGc0FBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "767b27f0-29c1-4672-cdc2-4dff45a2bfbc"
      },
      "source": [
        "execution_time "
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5243.575374126434"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6NWyuNog3hn"
      },
      "source": [
        "Low word counts indicate a problem extracting text from HTML. We can check how many records have a small word count as follows. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m1ydElvhLw8",
        "outputId": "60ce1355-8623-43d8-9a57-1bfe992c11c9"
      },
      "source": [
        "ind = output[\"Bag Of Words\"].str.len() < 100\n",
        "len(output[ind])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xZobn74bpFD"
      },
      "source": [
        "# Refferences\n",
        "\n",
        "Reporter, A. Daily Mail. “Daily Mail - Terms.” Mail Online, June 7, 2011. https://www.dailymail.co.uk/home/article-1388146/Terms.html."
      ]
    }
  ]
}